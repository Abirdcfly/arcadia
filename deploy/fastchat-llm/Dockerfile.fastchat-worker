FROM nvidia/cuda:12.2.0-devel-ubuntu20.04


# Define a build argument with a default value
ARG PACKAGE_REGISTRY="mirrors.tuna.tsinghua.edu.cn"

# Update the package registry based on the build argument
RUN sed -i "s/archive.ubuntu.com/$PACKAGE_REGISTRY/g" /etc/apt/sources.list \
    && sed -i "s/security.ubuntu.com/$PACKAGE_REGISTRY/g" /etc/apt/sources.list

# Configure the default Timezone
ENV TZ=Asia/Shanghai
RUN export DEBIAN_FRONTEND=noninteractive \
    && apt-get update \
    && apt-get install -y tzdata git \
    && ln -fs /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
    && dpkg-reconfigure --frontend noninteractive tzdata

# Official: https://pypi.org/simple
ARG PYTHON_INDEX_URL="https://pypi.mirrors.ustc.edu.cn/simple/"

# Install fastchat along with its dependencies
RUN apt-get install -y python3.9 python3.9-distutils curl python3-pip python3-dev gcc
RUN python3.9 -m pip install tomli setuptools_scm wavedrom transformers==4.37.0 -i ${PYTHON_INDEX_URL}
RUN python3.9 -m pip install --upgrade pip -i ${PYTHON_INDEX_URL}
RUN git clone https://github.com/lm-sys/FastChat.git \
    && cd FastChat \
    && python3.9 -m pip install -e ".[model_worker]"  -i ${PYTHON_INDEX_URL} \
    && git rev-parse HEAD > $HOME/.fastchat \
    && cd ..

# Configure the following environment variables to allow fastchat to pull model files from modelscope
# export VLLM_USE_MODELSCOPE=True ,export FASTCHAT_USE_MODELSCOPE=True
RUN python3.9 -m pip install modelscope pydantic==1.10.14 -i ${PYTHON_INDEX_URL}

# Install requirements for QWen(https://huggingface.co/Qwen/Qwen-72B-Chat)
RUN python3.9 -m pip install einops scipy transformers_stream_generator==0.0.4 deepspeed -i ${PYTHON_INDEX_URL}

# Install requirements for Qutantization with auto-gptq
RUN python3.9 -m pip install auto-gptq optimum -i ${PYTHON_INDEX_URL}

# Install requirements for vllm worker
# Ray v2.9.3 and vllm v0.3.3
RUN python3.9 -m pip install vllm==0.3.3 
RUN python3.9 -m pip install -U "ray[default]==2.9.3" -i ${PYTHON_INDEX_URL}

# Allow to use environment variable to set ray & python version to pass the version check
# for now, ray: 2.9.3, python: 3.9.x
# this utils.py file is from ray 2.9.0 ray-ml image
# search 'KubeAGI' in utils.py for what's changed
COPY deploy/llms/utils.py /usr/local/lib/python3.9/dist-packages/ray/_private/utils.py

COPY deploy/llms/start-worker.sh /
ENTRYPOINT ["/start-worker.sh"]
